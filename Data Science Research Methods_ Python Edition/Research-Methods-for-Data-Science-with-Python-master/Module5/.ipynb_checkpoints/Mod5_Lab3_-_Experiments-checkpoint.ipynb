{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 5, Lab 3: Between Subjects Experiments\n",
    "=============================================\n",
    "\n",
    "In this lab, we will explore the between-subjects experiment (sometimes\n",
    "referred to as an \"A/B\" test). I apply concepts from across the entire\n",
    "course, including review of statistical modeling, power analysis, effect\n",
    "size, measurement, and so on ... so that you can see the entire process\n",
    "at once.\n",
    "\n",
    "In this example, a design firm has produced several different logos for\n",
    "a company as part of a rebranding process, and the company executives\n",
    "have selected three that they like. These were then tested on a sample\n",
    "of 100 customers. Each participant completed a survey about a randomly\n",
    "selected logo. You are the data analyst and were called upon to make\n",
    "sense of the data.\n",
    "\n",
    "As part of the survey, participants rated their impression of the logo\n",
    "on several dimensions: \"friendly\", \"inviting\", \"interesting\",\n",
    "\"positive\", \"pleasant\" on a 1-10 scale (1 = *does not describe logo*; 10\n",
    "= *describes logo perfectly*). As these are all positive adjectives and\n",
    "you suspect people will not be very discerning in their answers, you\n",
    "wish to combine them into a single \"positive sentiment\" scale. You then\n",
    "wish to compare the logos on this sentiment scale.\n",
    "\n",
    "We will use a number of packages in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD PACKAGES ####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import statsmodels.stats.weightstats as ws\n",
    "from statsmodels.stats.power import tt_ind_solve_power\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data\n",
    "=============\n",
    "\n",
    "We first load the data and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATA ####\n",
    "dat = pd.read_csv(\"datasets/logos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly glance at the names of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  We have an ID variable representing the individual participants\n",
    "2.  We have variables representing the different adjectives on which\n",
    "    people rated the logo\n",
    "3.  We have participant sex\n",
    "4.  We have the logo that was rated\n",
    "\n",
    "It is clear this is not the whole dataset, as there would likely have\n",
    "been more variables.\n",
    "\n",
    "Let's briefly check the structure of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happily for us, we have the adjectives all scored as numeric variables\n",
    "and the `logo` variable scored as a categorical variable. We will need to do little\n",
    "(or nothing) to wrangle the data.\n",
    "\n",
    "We do a final quick inspection on the variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the ranges on the adjective variables are all in range\n",
    "(1-10), which we should expect (but should still check). The `logo` was\n",
    "randomly assigned to participants, so this is approximately evenly\n",
    "distributed. Annoyingly, we have a missing value on the `logo` variable.\n",
    "Of note,`sex` weights toward females, which we assume is representative\n",
    "of the data source. Hopefully this is representative of likely customers\n",
    "(if not, this would be a key limitation of this study).\n",
    "\n",
    "Score the Scale\n",
    "===============\n",
    "\n",
    "We had the goal of creating a \"positive sentiment\" variable for these\n",
    "data. We can quickly eyeball a correlation matrix to ensure these\n",
    "measures correlate highly with each other. The adjectives represent\n",
    "columns 2-6 of `dat`, so the correlations among the adjectives can be\n",
    "shown with that subset. Remember that the double brackets after `dat`\n",
    "contain contain the list of numeric variable columns. Note that I added the\n",
    "`round(3)` method to enhance readability, rounding to two decimals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[['friendly', 'inviting', 'interesting', 'positive', 'pleasant']].corr().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see these ratings are all moderately-strongly correlated. Later in\n",
    "your training, you might learn to run a principal components analysis or\n",
    "a factor analysis to ensure they appear to represent \"one underlying\n",
    "factor\" (i.e., sentiment). For now, we can be satisfied with showing\n",
    "acceptable correlations among the ratings and by showing acceptable\n",
    "scale reliability.\n",
    "\n",
    "Next, we proceed to score it as a sale. We use the Pandas `apply` method to apply the `mean` function from the numpy package to each row (`axis = 1`) to the numeric columns of the data frame to compute a`sentiment` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['sentiment'] = dat[['friendly', 'inviting', 'interesting', 'positive', 'pleasant']].apply(np.mean, axis = 1)\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see the sentiment variable in the data frame.\n",
    "\n",
    "Let's check this measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print some summary statistics\n",
    "print('Mean of Sentiment = ' + str(np.mean(dat.sentiment)))\n",
    "print('STD of Sentiment = ' + str(np.std(dat.sentiment)))\n",
    "\n",
    "## Plot a histogram\n",
    "ax = plt.figure(figsize=(8, 6)).gca() # define axis\n",
    "dat.sentiment.plot.hist(ax = ax, alpha = 0.6, bins = 15)\n",
    "plt.title('Histogram of Sentiment')\n",
    "plt.xlabel('Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that sentiment scores are skewed toward the positive end of the scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next question is tougher. Are we **measuring sentiment and\n",
    "not something else**, such as a tendency to respond positively on\n",
    "surveys? In this case, we don't know. The adjectives seem representative\n",
    "of the positive sentiment we would want to know about (i.e., content\n",
    "validity) and appear good on their face (i.e., face validity). However,\n",
    "we do not have evidence that this sent of adjectives teaks with other\n",
    "indicators of sentiment in this study. Thus, we are accepting on faith\n",
    "that these are a valid measure of sentiment. This is important to keep\n",
    "in mind when interpreting results. It would be ideal if we had a set of\n",
    "\"tried and true\" adjectives from prior studies that we could use that we\n",
    "*know* correlate in ways that show their validity. For now, we just have\n",
    "to trust the measure based on our impression.\n",
    "\n",
    "Dealing With Missing Values\n",
    "=====================================\n",
    "\n",
    "Before we proceed with more analysis we must determine if the data have missing values. In a Pandas data frame missing values can be coded in a number of ways, including as a numpy `nan` or as `none`, a null value. The code in the cell below tests which columns have these codings for missing  values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dat.columns:\n",
    "    print(col + ' has missing values ' + \n",
    "          str((dat[col].isnull().values.any()) or str(dat[col].isna().values.any())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to remove the missing values using the Pandas `dropna` method and test again to ensure we removed all the missing values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat.shape)\n",
    "dat.dropna(subset = ['logo'], inplace = True)\n",
    "print(dat.shape)\n",
    "\n",
    "## Check once more\n",
    "print('\\n')\n",
    "for col in dat.columns:\n",
    "    print(col + ' has missing values ' + \n",
    "          str((dat[col].isnull().values.any()) or str(dat[col].isna().values.any())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One row was removed from the data frame and there are no more missing values!\n",
    "\n",
    "Vizualize the Data + Run Descriptives\n",
    "=====================================\n",
    "\n",
    "The next step is to visualize the relationships in the data we want to\n",
    "test. We also should examine the descriptive statistics to test our\n",
    "question. In this case, we have randomly assigned participants to logos,\n",
    "so everything about the participants (age, sex, consumption history,\n",
    "etc.) will also be randomly distributed across the three logos. The\n",
    "*only* systematic differences across the three groups was the logo.\n",
    "Thus, any systematic differences in perceived friendliness can be\n",
    "attributed to the logo. This is the beauty of an experimental design. We\n",
    "can easily see the effect of the logo by comparing the groups.\n",
    "\n",
    "In the between-subjects design, we compare means across groups. Thus, we\n",
    "can visualize our data by looking at the distribution of scores across\n",
    "logo groups. The plot is created using the `boxplot` method from Seaborn along with the `swarmplot` method so we can see the individual data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8,8)).gca() # define axis\n",
    "sns.boxplot(x = 'logo', y = 'sentiment', data = dat, ax = ax)\n",
    "sns.swarmplot(x = 'logo', y = 'sentiment', color = 'black', data = dat, ax = ax, alpha = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod5_Lab3_-_Experiments_files/figure-markdown_strict/unnamed-chunk-11-1.png)\n",
    "We can see in our sample that Logo A and B appear to result in the\n",
    "highest positive sentiment, followed by Logo C. It's not clear whether\n",
    "Logo A or B is better.\n",
    "\n",
    "We can quickly compute some summary statistics. The Pandas `groupby` method groups the sentiment values by logo type and the mean is then computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo_grouped = dat[['logo','sentiment']].groupby('logo')\n",
    "\n",
    "print(' Mean by logo')\n",
    "print(logo_grouped.mean().round(2))\n",
    "print('\\n Standard deviation by logo')\n",
    "print(logo_grouped.std().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo A and B look close together and ahead of Logo C. Of course, that is\n",
    "just in our sample, so it is unclear whether we can infer there are real\n",
    "differences in the population. For that, we need our inferential tests.\n",
    "\n",
    "Inferential Test for Two Groups\n",
    "===============================\n",
    "\n",
    "In some experiments, you will have two groups to compare. For example,\n",
    "in the Module 3 lab on causal claims, we explored such an experiment. In\n",
    "others, you will have three or more groups. Although our overarching\n",
    "example is three groups, let's review this and pretend for a while that\n",
    "you were *only* comparing logos A and B to illustrate the two-group\n",
    "example.\n",
    "\n",
    "Whenever you compare (means of) two groups, you conduct an independent\n",
    "samples *t*-test. You can also use regression (run a regression\n",
    "predicting your outcome from your two-group predictor), but the *t*-test\n",
    "is commonly done and is illustrated here.\n",
    "\n",
    "Recall that Logo A did better *in our sample*, but is this a *real*\n",
    "difference in the population or just an artifact of random chance (due\n",
    "to reliance on random assignment to groups)?\n",
    "\n",
    "Recall that the null hypothesis always says that the **effect is absent\n",
    "in the population** and that the sample result is an artifact of random\n",
    "chance. In symbols, this means that the difference between the group\n",
    "averages is exactly zero in the population.\n",
    "\n",
    "$$H_0:\\ \\mu_A - \\mu_B = 0$$\n",
    "\n",
    " Remember that *μ* refers to the population average, so this is saying\n",
    "that the population difference is exactly zero. Any difference observed\n",
    "in our sample is therefore due to random chance.\n",
    "\n",
    "We run our *t*-test to consider this possibility.\n",
    "\n",
    "Recall that a *t*-test compares the size of the *observed difference*\n",
    "($\\bar{x}_{1}-\\bar{x}_{2}$) against the value in the null hypothesis\n",
    "(zero), divided by what is typically expected by chance:\n",
    "\n",
    "$$t=\\frac{result - null }{chance}$$  \n",
    "\n",
    "The top of the faction is key here. The more the data \"disagree\" with\n",
    "the null, the larger the *t*-value and hence more evidence for a logo\n",
    "effect. However, we also know some differences occur due to random\n",
    "chance, so we divide by a measure of that that to take it into account.\n",
    "When all is said and done, therefore, a large *t*-value tells you that\n",
    "the effect is considerably larger than expected by chance. That would be\n",
    "evidence for a real logo effect.\n",
    "\n",
    "How can we run our test? The default in R is to run the \"Welch\" version\n",
    "of the test. This version of the test does *not* make any assumptions\n",
    "about the variances of the two groups.\n",
    "\n",
    "$$t'=\\frac{result - null }{chance}=\\frac{(\\bar{x}_{1}-\\bar{x}_{2}) - 0 }{\\sqrt{\\frac{\\hat{\\sigma}_1^2}{n_{1}}+\\frac{\\hat{\\sigma}_2^2}{n_{2}}}}$$\n",
    "\n",
    "There's no need to worry about the equation; it just implements what we\n",
    "said above. Notice that the bottom includes the sample variances of the\n",
    "two groups ($\\hat{\\sigma}_1^2$ and $\\hat{\\sigma}_2^2$ .... i.e.,\n",
    "standard deviations squared). These are kept separate in the Welch\n",
    "*t*-test, meaning that it's OK if they are different from each other.\n",
    "\n",
    "The function below computes the t-test values. Several methods from different packages are used. The two subsets of the data frame are created using the `loc` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_two_samp(a, b, alpha, alternative='two-sided'):\n",
    "    \n",
    "    diff = a.mean() - b.mean()\n",
    "\n",
    "    res = ss.ttest_ind(a, b)\n",
    "      \n",
    "    means = ws.CompareMeans(ws.DescrStatsW(a), ws.DescrStatsW(b))\n",
    "    confint = means.tconfint_diff(alpha=alpha, alternative=alternative, usevar='unequal') \n",
    "    degfree = means.dof_satt()\n",
    "\n",
    "    index = ['DegFreedom', 'Difference', 'Statistic', 'PValue', 'Low95CI', 'High95CI']\n",
    "    return pd.Series([degfree, diff, res[0], res[1], confint[0], confint[1]], index = index)   \n",
    "   \n",
    "\n",
    "test = t_test_two_samp(dat.loc[dat.logo == 'Logo A', 'sentiment'], dat.loc[dat.logo == 'Logo B', 'sentiment'], 0.05)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *t*-value of 0.38 tells us that the difference is 0.38 times the\n",
    "size of what one would typically expect by chance. This gives us our\n",
    "*p*-value. We see here that the *p*-value is .70 so by definition, we\n",
    "could get a result this big in our sample 70% of the time when the null\n",
    "hypothesis that there is *NO* difference is true. This is not\n",
    "statistically significant (*p* must be below .05 or 5%). Further, we see the confidence interval includes 0. We must accept\n",
    "the null hypothesis that this could plausibly be due to random chance. I\n",
    "don't have enough evidence to declare a real difference between Logo A\n",
    "and Logo B.\n",
    "\n",
    "#### What are the variance assumptions?\n",
    "\n",
    "Note that there is another version of the *t*-test, Student's *t*, which\n",
    "is the same but it uses a form of averaging of the two sample variances:\n",
    "\n",
    "$$t=\\frac{result - null }{chance}= \\frac{(\\bar{x}_{1}- \\bar{x}_{2}) - 0 }{\\sqrt{\\hat{\\sigma}_p^2 (\\frac{1}{n_{1}}+\\frac{1}{n_{2}}))}}$$  \n",
    "\n",
    "In this case, the $\\hat{\\sigma}_p^2$ is a weighted average of the\n",
    "two sample variances. If you cared for the equation for that (which you\n",
    "probably don't), it looks like this:\n",
    "\n",
    "$$\\hat{\\sigma}_p^2 = \\frac{\\left( n_1 - 1 \\right)\\hat{\\sigma}_1^2+\\left( n_2-1 \\right)\\hat{\\sigma}_2^2}{\\left( n_1 - 1 \\right)+\\left( n_2-1 \\right)}$$  \n",
    "\n",
    "Essentially, this is a sophisticated form of averaging\n",
    "$\\hat{\\sigma}_1^2$ and $\\hat{\\sigma}_2^2$ so that the one from the\n",
    "larger group has more influence over the final answer.\n",
    "\n",
    "Long story short: this may not be the best test, as it *requires* the\n",
    "variances (*σ*<sub>1</sub><sup>2</sup> and *σ*<sub>2</sub><sup>2</sup> )\n",
    "of the two groups to be equal at the population level. Since we can\n",
    "never know anything about the population, we can never know if this\n",
    "assumption is met. We can test for it, but that has its own\n",
    "complications. Further, recent advice (Delacre, Lakens, & Leys, 2017)\n",
    "indicates that the Welch's *t* (done above) is better under real-world\n",
    "conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "We need to tell our team that Logo A and Logo B are statistically tied:\n",
    "we have no evidence for any real difference between them at the\n",
    "population level. But what does that mean?\n",
    "\n",
    "Before we move on: remember that **any** time a result is not\n",
    "significant, it means that either\n",
    "\n",
    "1.  The null hypothesis is true (i.e., the two groups are **exactly**\n",
    "    equal in the population)\n",
    "2.  The effect is smaller than we have power to detect\n",
    "\n",
    "In fact, given our sample size, it's possible that there may still be an\n",
    "logo effect...and we simply missed it. Given our sample size, what can\n",
    "we actually *say* about the difference? This is given for us in the\n",
    "*t*-test output with a 95% Confidence Interval.\n",
    "\n",
    "Notice in the middle of the *t*-test output that a 95% CI is given to\n",
    "you: \\[-0.55 to 0.82\\]. This means that we are 95% confident that the\n",
    "true difference between the groups is somewhere between a half a point\n",
    "*lower* for Logo A and nearly a full point *higher* for Logo A. Thus, it\n",
    "is obvious this is non-significant (since a difference of *zero* is\n",
    "right in the middle of that range of plausible values).\n",
    "\n",
    "It is up to our subject matter expertise to interpret this. On a 1-10\n",
    "satisfaction scale, we are confident that the they are within about .50\n",
    "to .80 points of each other. That's not a very big difference in my\n",
    "subjective opinion. Thus, we can reasonably state that there is not a\n",
    "large difference between our groups, but it is possible there is still a\n",
    "small difference (specifically: between a half a point *lower* for Logo\n",
    "A and nearly a full point *higher* for Logo A).\n",
    "\n",
    "What if we wanted a more precise answer with a tighter range? Well, we\n",
    "would need a bigger sample size. A bigger sample has less error and\n",
    "therefore a more precise CI. If our higher ups wanted a more precise\n",
    "answer, we would need to re-run the survey with a larger sample.\n",
    "\n",
    "#### If Units Are Meaningless\n",
    "\n",
    "In this case, a 1-10 satisfaction scale has somewhat meaningful units.\n",
    "In some situations, this is not the case. In such situations, you can use the\n",
    "Cohen's *d* effect size statistic.\n",
    "\n",
    "Recall from an earlier module that this quantifies the size of the\n",
    "difference using a standard unit. Technically, that unit is standard\n",
    "deviations:\n",
    "$d = \\frac{difference}{SD} = \\frac{\\bar{x}_{1}-\\bar{x}_{2}}{\\sigma}$.\n",
    "If you don't care about formulas, you simply need to know that it\n",
    "rephrases the size of the difference using a common unit with a known\n",
    "scale to it:\n",
    "\n",
    "|        | d Value          | Meaning  |\n",
    "| ------------- |:-------------:|-----------:|\n",
    "| 1.  | 0 - 0.2 | Negligible |\n",
    "| 2.  | 0.2 - 0.5     |  Small |\n",
    "| 3. | 0.5 - 0.8      |  Medium |\n",
    "| 4. | 0.80 +      |  Large |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we should find the number of samples in each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo_grouped.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the mater of the small d value for the difference between Logo A and Logo B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = (8.58 - 8.44)/(np.std(dat.loc[dat.logo == 'Logo A', 'sentiment']))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to compute the power of the t-test test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_ind_solve_power(effect_size=d, nobs1 = 32, alpha=0.05, power=None, ratio=1, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the difference between them in the sample was classified as\n",
    "\"negligible.\" Further, this gives you the same 95% CI from earlier, but\n",
    "in Cohen's *d* units instead of \"points\" on the satisfaction scale.\n",
    "This leaves a test with rather low power.\n",
    "\n",
    "#### Final Report\n",
    "\n",
    "Let's see how we might summarize our final conclusions. Note that this\n",
    "would be written differently depending on who the audience is (i.e.,\n",
    "board members, other data scientists). I present the most frank\n",
    "(non-hyped-up) version of the analysis here.\n",
    "\n",
    "First, I would begin with a visualization of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(8,8)).gca() # define axis\n",
    "temp = dat[dat.logo != 'Logo C']\n",
    "sns.boxplot(x = 'logo', y = 'sentiment', data = temp, ax = ax)\n",
    "sns.swarmplot(x = 'logo', y = 'sentiment', color = 'black', data = temp, ax = ax, alpha = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod5_Lab3_-_Experiments_files/figure-markdown_strict/unnamed-chunk-20-1.png)\n",
    "\n",
    "Second, I present the results. Generally, I try to state what we\n",
    "observed first, quantifying the size of the observed results. Second, I\n",
    "try to state something about whether the effect is significant, using\n",
    "common language where possible. Third, I try to present some measure of\n",
    "uncertainty (e.g., our CI), interpreting in plain language as much as\n",
    "possible. Here is a paragraph possibility:\n",
    "\n",
    "> The average sentiment for Logo A was slightly higher (*M* = 8.57, *SD*\n",
    "> = 1.27) than the average sentiment for Logo B (*M* = 8.44, *SD* =\n",
    "> 1.51). This was a small difference...only 0.13 points on the 1-10\n",
    "> scale. Given our sample size, this was well within what would be\n",
    "> expected by random chance (i.e., not statistically significant; *t* =\n",
    "> 0.38, *p* = 0.70). Thus, they were statistically tied.\n",
    "\n",
    "> It is possible that a small sentiment difference may still exist\n",
    "> between the two logos. We are 95% confident that the difference is\n",
    "> somewhere between 0.80 points higher for Logo A and 0.55 pionts higher\n",
    "> for Logo B. So, if they do differ, they are likely very close to one\n",
    "> another; however, we woudl need a larger sampel to say something more\n",
    "> precise than that.\n",
    "\n",
    "#### Addendum: Power Considerations\n",
    "\n",
    "We are done with the analysis. However, it's worth briefly chatting\n",
    "about power. Did our study have much ability to detect a difference\n",
    "between Logo A and Logo B in the first place? Was it set up to fail?\n",
    "\n",
    "Recall that power depends on both sample size and effect size. Large\n",
    "samples can detect small effects. Smaller samples cannot detect small\n",
    "effects.\n",
    "\n",
    "We simply feed information about our study (our sample size and request\n",
    "for 80% power). We can compute the smallest effect we could reliably detect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_ind_solve_power(effect_size=None, nobs1 = 32, alpha=0.05, power=0.8, ratio=1, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha. We could **ONLY** reliably detect effects as small as *d* = .71 ...\n",
    "a \"moderate-large\" effect. So, this reinforces our conclusion above...\n",
    "we would have detected a moderate-large difference between Logo A and B,\n",
    "but if there was a smaller difference, we were prone to miss it.\n",
    "\n",
    "Why bring this up? Because I want to remind you that a non-significant\n",
    "result *does not mean the effect was exactl zero*. It would be a mistake\n",
    "to say \"there was NO difference between Logo A and Logo B.\" Instead, as\n",
    "we stated above, we should say \"there was not a large difference between\n",
    "Logo A and Logo B.\" This reinforces that, but you wouldn't need to\n",
    "report this.\n",
    "\n",
    "However, this *would* be worth doing prior to running the study as we\n",
    "plan our sample size. What if we wanted to be able to detect a smaller\n",
    "difference, say *d* = .20 with our 80% power? In this case, we use the d value that must be achieved \n",
    "*d* value and leave out the number of samples which we want to know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_ind_solve_power(effect_size=0.2, nobs1 = None, alpha=0.05, power=0.8, ratio=1, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we would need nearly 393 people *per logo* to detect\n",
    "small differences. If we were planning the study from the start, we\n",
    "might iteratively explore several possible sample sizes to find the\n",
    "optimal balance between sample size (i.e., cost of the study) and\n",
    "ability to detect an effect.\n",
    "\n",
    "Example 1: Inferential Test for Three Groups\n",
    "============================================\n",
    "\n",
    "The previous example considered only two groups: Logo A and Logo B.\n",
    "However, we had three logos in our study.\n",
    "\n",
    "When three or more groups are present, we switch from a *t*-test to\n",
    "*ANOVA* (ANalysis Of VAriance).\n",
    "\n",
    "The ANOVA is a two stage analysis. In the first stage, we test whether\n",
    "we can the logo mattered at *all* (i.e., null hypothesis says there were\n",
    "*no* differences:\n",
    "*H*<sub>0</sub> : *μ*<sub>*A*</sub> = *μ*<sub>*B*</sub> = *μ*<sub>*C*</sub>).\n",
    "If that comes up significant, we can systematically compare the logos.\n",
    "\n",
    "In this example, I assume you are familiar with ANOVA. An overview of\n",
    "the math behind ANOVA goes beyond the scope of this lab. Essentially,\n",
    "ANOVA computes the *F* statistic, which yields a *p*-value for the null\n",
    "hypothesis that all the groups are equal. If we can reject that null\n",
    "hypothesis, it means the logo is doing something. I illustrate this\n",
    "next.\n",
    "\n",
    "The `f_oneway` function from the scipy.stats package computes the F-Statistic and p-value for two or more groups. In this case, the groups are subsets defined by the three logos being tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_statistic, p_value = ss.f_oneway(dat.loc[dat.logo == 'Logo A', 'sentiment'],\n",
    "                                   dat.loc[dat.logo == 'Logo B', 'sentiment'],\n",
    "                                   dat.loc[dat.logo == 'Logo C', 'sentiment'])\n",
    "print('F-Satatistic = ' + str(f_statistic))\n",
    "print('p_value = ' + str(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first step tests the null hypothesis that *all the group means are\n",
    "equall* ... in other words that the logo doesn't matter **at all**. In\n",
    "this case, the *p* value is 6.25e-06 (aka 0.00000625). This is highly\n",
    "statistically significant (*p* &lt; .05). By definition, this means that\n",
    "we could only get a logo effect this big 0.000625% of the time by random\n",
    "chance alone. So we are confident the logo is doing something.\n",
    "\n",
    "Why did we bother with the test described above? Consider what would\n",
    "happen had we done several *t*-tests. There were three possible\n",
    "*t*-tests we could have conducted, each of which has the possibility of\n",
    "giving us a false positive. If we were to run *many* tests, we would\n",
    "likely stumble across a false-positive eventually. Thus, we appreciate\n",
    "the ability to conduct a **single** overall test to tell us our logo is\n",
    "doing something.\n",
    "\n",
    "The next step is to figure out where the differences are. Recall we made\n",
    "a table of results earlier with using the Pandas `groupby` and `mean()` methods.\n",
    "\n",
    "To avoid the issue described above, we use a special test (there are\n",
    "many; I cover Tukey's HSD here) that takes into account the number of\n",
    "comparisons we want to make. We wish to make 3 comparisons:\n",
    "\n",
    "1.  A vs. B\n",
    "2.  A vs. C\n",
    "3.  B vs. C\n",
    "\n",
    "Tukey's test knows how many comparisons we want to make. It inflates our\n",
    "*p*-values on each test, making it harder to detect a difference for any\n",
    "one comparison. This *reduces* our likelihood of making a false positive\n",
    "on any one comparison, counteracting the problem of running multiple\n",
    "tests. Neat!\n",
    "\n",
    "It is easy to run. The `pairwise_tukeyhsd` from the statsmodels.stats.multicomp package computes these differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tukey_HSD = pairwise_tukeyhsd(dat.sentiment, dat.logo)\n",
    "print(Tukey_HSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above also gave us difference scores + 95% CIs. Very handy:\n",
    "\n",
    "1.  Group A vs. B = not significant. Logo B was 0.13 points lower than Logo A. We are 95% confident the\n",
    "    true difference is between -0.94 and 0.68. This includes zero, so we\n",
    "    cannot declare a difference between Logo B and Logo A.\n",
    "\n",
    "2.  Group A vs. C = highly significant. Logo C was 1.58 points lower than Logo A. We are 95% confident the\n",
    "    true difference is between -2.37 and -0.78.\n",
    "\n",
    "3.  Group B vs C = highly significant. Logo C was 1.44 points lower than Logo A. We are 95% confident the\n",
    "    true difference is between -2.25 and -0.64.\n",
    "\n",
    "As in the *t*-test example above, we see the confidence intervals are\n",
    "helpful. If we wanted more precise CIs, we need a larger sample.\n",
    "\n",
    "We can see these confidence intervals for each of the means visually by applying the `plot_simultaneous` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tukey_HSD.plot_simultaneous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Report\n",
    "\n",
    "A report might look as follows:\n",
    "\n",
    "> The average sentiment on a 1-10 scale is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo_grouped = dat[['logo','sentiment']].groupby('logo')\n",
    "\n",
    "print(' Mean by logo')\n",
    "print(logo_grouped.mean().round(2))\n",
    "print('\\n Standard deviation by logo')\n",
    "print(logo_grouped.std().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An ANOVA revealed a signifiant effect of logo on sentiment (*F* =\n",
    "> 13.61, *p* &lt; .001). As shown in the table above, Logos A and B were\n",
    "> similar in sentiment, while Logo C had reduced sentiment. The pattern\n",
    "> of results was such that Logos A and B were statistically tied, and\n",
    "> both performed better than Logo C. Further, the confidence intervals computed with the Tukey HSD test supports this conclusion.\n",
    "\n",
    "Given the number of results, one could then report the CIs or *p*-values\n",
    "above in a table or in paragraph form. It is possible to go into the\n",
    "same level of detail as for the *t*-test, but people usually don't, as\n",
    "it becomes exponentially more complicated with all the comparisons going\n",
    "on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
