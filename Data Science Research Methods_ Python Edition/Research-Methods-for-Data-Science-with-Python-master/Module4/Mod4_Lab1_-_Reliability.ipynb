{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 4, Lab 1 - Reliability\n",
    "=============================\n",
    "\n",
    "In this lab, we will explore the idea of reliability. Reliability is\n",
    "about consistency in measurement, and it is a bare minimum in order for\n",
    "a measure to be sound. After all, if you're not consistently measuring\n",
    "something, how can you say your measures are trustworthy?\n",
    "\n",
    "In this lab, we will use the `psych` package for psychometrics (e.g.,\n",
    "reliability analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATA ####\n",
    "install.packages('psych')\n",
    "library(psych)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are given a questionnaire developed by a the executives of a\n",
    "taco company. They want to know how people feel about their brand. In a\n",
    "survey, 200 people were asked their impressions of the taco brand on\n",
    "five adjectives representing core themes used in the branding process\n",
    "and agreed upon by the executive board of the company. These themes are\n",
    "meant to be distinctives of the brand. Every participant rated how much\n",
    "each adjective describes the brand on a 1 (not at all) to 10\n",
    "(completely) scale. The company has suggested the participant score can\n",
    "form a \"brand value\" index.\n",
    "\n",
    "1.  Inviting\n",
    "2.  Friendly\n",
    "3.  Awesome\n",
    "4.  Quirky\n",
    "5.  Pleasant\n",
    "\n",
    "You seek to know whether these questions reliably and consistently\n",
    "measure brand sentiment. To answer that question, we must first ask\n",
    "three questions:\n",
    "\n",
    "1.  Are the answers to these questions interrelated\n",
    "2.  If so, there evidence that these items seem to be measuring \"one\n",
    "    thing\" or \"more than one thing\"\n",
    "3.  If they are measuring one thing, do they do so reliably and how can\n",
    "    we optimize reliability?\n",
    "\n",
    "Start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LOAD DATA ####\n",
    "dat <- read.csv(\"datasets/measurement.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should check the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(dat)\n",
    "\n",
    "##        id            friendly         inviting         awesome      \n",
    "##  Min.   :  1.00   Min.   : 4.455   Min.   : 3.092   Min.   : 3.221  \n",
    "##  1st Qu.: 50.75   1st Qu.: 6.860   1st Qu.: 6.800   1st Qu.: 6.656  \n",
    "##  Median :100.50   Median : 7.960   Median : 7.873   Median : 8.215  \n",
    "##  Mean   :100.50   Mean   : 7.917   Mean   : 7.796   Mean   : 7.899  \n",
    "##  3rd Qu.:150.25   3rd Qu.: 9.080   3rd Qu.: 9.047   3rd Qu.: 9.255  \n",
    "##  Max.   :200.00   Max.   :10.000   Max.   :10.000   Max.   :10.000  \n",
    "##      quirky         pleasant     \n",
    "##  Min.   : 8.71   Min.   : 3.976  \n",
    "##  1st Qu.:10.00   1st Qu.: 6.832  \n",
    "##  Median :10.00   Median : 7.793  \n",
    "##  Mean   : 9.97   Mean   : 7.809  \n",
    "##  3rd Qu.:10.00   3rd Qu.: 9.167  \n",
    "##  Max.   :10.00   Max.   :10.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immediate question before us is whether these five adjectives\n",
    "reliably form a single index of anything.\n",
    "\n",
    "The first task is to check the correlations among the items. To make the\n",
    "code easier, we can briefly drop the `id` variable. All of the commands\n",
    "from here on out require that we input a data frame that has **only**\n",
    "the items we wish to analyze. You might also make a new dataframe for\n",
    "just the items you want if you are working with a larger set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat <- dat[,-1]\n",
    "names(dat)\n",
    "\n",
    "## [1] \"friendly\" \"inviting\" \"awesome\"  \"quirky\"   \"pleasant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can run `cor()` on the measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(cor(dat), 2)\n",
    "\n",
    "##          friendly inviting awesome quirky pleasant\n",
    "## friendly     1.00     0.81    0.80   0.30     0.81\n",
    "## inviting     0.81     1.00    0.70   0.22     0.64\n",
    "## awesome      0.80     0.70    1.00   0.23     0.64\n",
    "## quirky       0.30     0.22    0.23   1.00     0.27\n",
    "## pleasant     0.81     0.64    0.64   0.27     1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that most of the items are strongly inter-correlated, except\n",
    "for `quirky`. This question doesn't' seem to fit with the others. We may\n",
    "flag this as a possible odd item.\n",
    "\n",
    "One Dimension?\n",
    "==============\n",
    "\n",
    "We can break down the reliability test into two stages. In the first\n",
    "stage, we can ask whether we are measuring \"one thing\" or several things\n",
    "with this set of questions. *After all, you cannot have a reliable\n",
    "measure of something unless you know you are measuring one thing to\n",
    "begin with.* Although many people skip this step, I illustrate it here.\n",
    "\n",
    "A factor analysis (similar to a \"principal component analysis,\" which\n",
    "you will learn later) can tell us how many underlying dimensions appear\n",
    "to be operating beneath this set of variables. For example, if I asked\n",
    "someone feeling angry if they feel \"upset,\" \"mad,\" and \"angry\" ... I\n",
    "would likely get similar answers because these questions are all\n",
    "assessing the **one** underlying dimension of anger. There does not\n",
    "appear to be a separate dimension beneath those items (we call questions\n",
    "\"items\" in survey writing). Similarly, we need to know if this set of\n",
    "questions we have asked represents one underlying dimension or several.\n",
    "We call these underlying dimensions \"factors.\" Thus, we need to know how\n",
    "many factors we have.\n",
    "\n",
    "I won't explain the details of factor analysis here (beyond the scope of\n",
    "this course), but it essentially analyzes the patterns of association\n",
    "between the items to determine if they \"cluster together\" and if so,\n",
    "how. We start with `fa.parallel()` from the `psych` package. We tell it\n",
    "we want to use the `minres` factor method (`fm = 'minres'`), which makes\n",
    "few assumptions about the normality of the data, and we tell it we want\n",
    "the analysis to be a \"factor analysis\" (`fa = 'fa'`).\n",
    "\n",
    "I won't go into details about the analysis, but we compute a number of\n",
    "\"eigenvalues\" for our data. Historically, the number of eigenvalues\n",
    "above 1.0 was seen as the number of factors in the data. For example, if\n",
    "we had 3 eigenvalues above 1.0, that would be evidence that our\n",
    "questions measured 3 underlying dimensions.\n",
    "\n",
    "Modern techniques involve a \"parallel analysis,\" in which the computer\n",
    "generates random data (based on your data) but with *no* underlying\n",
    "factors. Where your data differ from the simulated data, you have\n",
    "evidence for a factor. We count the number of times that occurs, and\n",
    "that is the number of factors you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa.parallel(dat, fm = 'minres', fa = 'fa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod4_Lab1_-_Reliability_files/figure-markdown_strict/unnamed-chunk-6-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parallel analysis suggests that the number of factors =  1  and the number of components =  NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computer here is telling you that we see evidence for one factor.\n",
    "This is supported by the graph. The red lines in the graph represent the\n",
    "results from the random computer-generated data, whereas the blue line\n",
    "represents your data. We see that, in the column labeled \"Factor Number\n",
    "1,\" the blue line and red lines diverge--evidence for a factor! Further,\n",
    "the \"eigenvalue\" for that column is above 1.0 (height of blue triangle\n",
    "on the y-axis).\n",
    "\n",
    "For the remaining columns on the graph, we see that our data (blue line)\n",
    "gives identical results to the simulated random data (red lines). Thus,\n",
    "we have no evidence for any additional factors. Therefore, we can\n",
    "conclude that we have one underlying dimension to our data. This means\n",
    "we are measuring \"one thing\" with this set of questions.\n",
    "\n",
    "Next, we can can focus in on our questions and consider whether our\n",
    "items each correlate well with the \"one underlying dimension.\" A good\n",
    "question is conceptually close to the underlying dimension and therefore\n",
    "does not \"pick up\" on a lot of other junk. For example, the \"quirky\"\n",
    "question has a possible negative connotation to it; it may inadvertently\n",
    "have more going on with it than simply assessing positive / negative\n",
    "sentiment. Thus, I would not be surprised if it fails to correlate as\n",
    "well with the underlying factor. We call these correlations \"loadings.\"\n",
    "\n",
    "We can run this more detailed factor analysis using the `fa()` command\n",
    "from the `psych` package, specifically asking it to retrieve one factor.\n",
    "We then look at the \"loadings.\" Good questions should be highly\n",
    "correlated to our underlying dimension. In general, I would be wary of\n",
    "anything in the .3-.5 range or below. These loading are given in the\n",
    "column labeled `MR1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa(dat, nfactors = 1, fm=\"minres\")$loadings\n",
    "\n",
    "## \n",
    "## Loadings:\n",
    "##          MR1  \n",
    "## friendly 0.992\n",
    "## inviting 0.818\n",
    "## awesome  0.812\n",
    "## quirky   0.299\n",
    "## pleasant 0.805\n",
    "## \n",
    "##                  MR1\n",
    "## SS loadings    3.048\n",
    "## Proportion Var 0.610"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: you can get much more detailed analyses if you omit the\n",
    "`$loadings` code, but you won't be able to make sense of it without a\n",
    "richer understanding of factor analysis, which is beyond this tutorial).\n",
    "\n",
    "We see here that the \"friendly\", \"inviting,\" \"awesome,\" and \"pleasant\"\n",
    "questions are highly correlated (~.80) with the underlying factor. This\n",
    "is good news, because these questions are tracking closely with the\n",
    "underlying dimension. However, not surprisingly, \"quirky\" does not load\n",
    "as well. In fact, the correlation is weak, loading at only .299. At this\n",
    "point, I would consider analyzing quirky separately or removing it from\n",
    "the measure entirely.\n",
    "\n",
    "There is one point worth mentioning. Factor analyses almost always\n",
    "overfit the data (i.e., they will model the sample data perfectly).\n",
    "Thus, they are often worth replicating in a separate set of data, if\n",
    "possible. It's sometimes useful to split the data in half, for example,\n",
    "training the model in one set of data and cross-validating in the other.\n",
    "\n",
    "Reliability\n",
    "===========\n",
    "\n",
    "Next, we can assess the reliability of the measure. Technically defined,\n",
    "reliability is \"the percentage of variance in the scores due to the\n",
    "thing you're measuring\" (i.e., due to the factor). If reliability is\n",
    "100%, then our measure is perfectly reliable and every bump in the data\n",
    "is real variation in sentiment. If reliability is less than 100%--for\n",
    "example, 80%--then most of the variation is due to the thing you are\n",
    "measuring but the remaining 20% is measurement error, other random\n",
    "unrelated junk that you do not care about and is watering down your\n",
    "measure.\n",
    "\n",
    "I will point out that there are trade-offs in measurement. You can get a\n",
    "perfectly reliable measure by asking the same question several different\n",
    "ways, but that is redundant and frankly uninformative. It's worth it to\n",
    "ask questions slightly differently (e.g., words such as \"friendly\" and\n",
    "\"awesome\" are both positive words but not identical). You add more\n",
    "information to your measure that way, but the trade off is that\n",
    "reliability can suffer. In general, reliability between .70 and .90 is\n",
    "good, with .80-.95 preferred.\n",
    "\n",
    "To run our test, we use `alpha()` from the `psych` package. Since both\n",
    "`ggplot2` and `psych` have commands with that name, it's useful to\n",
    "preface the command with `psych::` to be sure that it pulls the command\n",
    "from `psych`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych::alpha(dat)\n",
    "\n",
    "## \n",
    "## Reliability analysis   \n",
    "## Call: psych::alpha(x = dat)\n",
    "## \n",
    "##   raw_alpha std.alpha G6(smc) average_r S/N    ase mean  sd\n",
    "##       0.86      0.86    0.87      0.54 5.9 0.0098  8.3 1.1\n",
    "## \n",
    "##  lower alpha upper     95% confidence boundaries\n",
    "## 0.84 0.86 0.88 \n",
    "## \n",
    "##  Reliability if an item is dropped:\n",
    "##          raw_alpha std.alpha G6(smc) average_r  S/N alpha se\n",
    "## friendly      0.77      0.77    0.75      0.45  3.3    0.016\n",
    "## inviting      0.80      0.81    0.82      0.51  4.1    0.012\n",
    "## awesome       0.81      0.81    0.82      0.51  4.1    0.012\n",
    "## quirky        0.91      0.92    0.91      0.73 10.9    0.011\n",
    "## pleasant      0.81      0.80    0.82      0.51  4.1    0.011\n",
    "## \n",
    "##  Item statistics \n",
    "##            n raw.r std.r r.cor r.drop mean   sd\n",
    "## friendly 200  0.95  0.93  0.96   0.92  7.9 1.35\n",
    "## inviting 200  0.88  0.85  0.82   0.78  7.8 1.54\n",
    "## awesome  200  0.88  0.84  0.81   0.77  7.9 1.70\n",
    "## quirky   200  0.31  0.51  0.30   0.29 10.0 0.14\n",
    "## pleasant 200  0.87  0.85  0.82   0.76  7.8 1.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top row gives us what we want:\n",
    "\n",
    "`raw_alpha`  \n",
    "`0.86`\n",
    "\n",
    "Our measure is an estimated 86% reliable, or only 14% measurement error.\n",
    "This row also tells us that the average correlation among our items is\n",
    ".54. Can we do better?\n",
    "\n",
    "However, this included `quirky`. We see below the top row is the line,\n",
    "`Reliability if an item is dropped:` which reports that dropping\n",
    "`quirky` would actually improve our reliability to .91. This row also\n",
    "tells us that the average correlation among our questions would increase\n",
    "to .73.\n",
    "\n",
    "This confirms our gut intuition, our factor analysis, and now our\n",
    "reliability analysis. I would drop the quirky item (and report it\n",
    "separately if desired).\n",
    "\n",
    "You can quickly score the scale using `rowSums()`, which accepts a\n",
    "`data.frame()` of the desired variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat$sentiment <- rowSums(data.frame(dat$friendly, dat$inviting, dat$awesome, dat$pleasant))\n",
    "\n",
    "hist(dat$sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Mod4_Lab1_-_Reliability_files/figure-markdown_strict/unnamed-chunk-9-1.png)\n",
    "\n",
    "We still have not assessed whether the thing we are measuring *is*\n",
    "sentiment. For example, maybe it's just measuring enthusiasm in filling\n",
    "out survey questions! Still, it's a start. We can next explore the\n",
    "validity of the measure, which we will do in the next lab.\n",
    "\n",
    "Future Directions\n",
    "=================\n",
    "\n",
    "This is but a first taste of measurement development. There is an entire\n",
    "field known as \"psychometrics\" with many fantastic tools for measurement\n",
    "development. Good measurement is hard, but without good measurement, our\n",
    "data are wrong. Further, some cases misleading data can be worse than no\n",
    "data. Taking the time to assess the reliability and validity of measures\n",
    "is therefore vital to doing good data science whenever measures must be\n",
    "constructed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
